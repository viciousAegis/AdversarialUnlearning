{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshitsinha3/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/Projects/GraphUnlearning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch  # pytorch backend\n",
    "from grb.dataset import Dataset\n",
    "from grb.model.torch import GCN\n",
    "from grb.utils.trainer import Trainer\n",
    "from grb.attack.tdgia import TDGIA\n",
    "import torch  # pytorch backend\n",
    "from grb.dataset import Dataset\n",
    "from grb.model.torch import GCN\n",
    "from grb.utils.normalize import GCNAdjNorm\n",
    "from grb.defense import AdvTrainer\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cora\n",
    "dataset_name = 'Cora'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Planetoid(root='data', name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grb.dataset import CustomDataset\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom dataset from planetoid dataset\n",
    "\n",
    "adj_matrix = to_scipy_sparse_matrix(dataset[0].edge_index)\n",
    "features = dataset[0].x\n",
    "labels = dataset[0].y\n",
    "name = 'cora-custom'\n",
    "data_dir = 'grb_data/cora'\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRB data splitting...\n",
      "    Average degree of all nodes: 3.8981\n",
      "    Average degree of 5% nodes with small degree: 1.0000\n",
      "    Average degree of 5% nodes with large degree: 17.4265\n",
      "    Average degree of 30% nodes (easy): 1.5690\n",
      "    Randomly sampled 270 nodes\n",
      "    Average degree of 30% nodes (medium): 3.0221\n",
      "    Randomly sampled 270 nodes\n",
      "    Average degree of 30% nodes (hard): 5.3202\n",
      "    Randomly sampled 270 nodes\n",
      "    Number of training/validation nodes: 1624/274\n",
      "    No duplicate.\n",
      "    Saved in grb_data/cora.\n",
      "Custom Dataset 'cora-custom' loaded.\n",
      "    Number of nodes: 2708\n",
      "    Number of edges: 5278\n",
      "    Number of features: 1433\n",
      "    Number of classes: 7\n",
      "    Number of train samples: 1624\n",
      "    Number of val samples: 274\n",
      "    Number of test samples: 810\n",
      "    Dataset mode: full\n",
      "    Feature range [0.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "cora_custom = CustomDataset(\n",
    "    adj=adj_matrix,\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    name=name,\n",
    "    data_dir=data_dir,\n",
    "    save=save,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x2708 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 10556 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_custom.adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'grb-cora' loaded.\n",
      "    Number of nodes: 2708\n",
      "    Number of edges: 5278\n",
      "    Number of features: 1433\n",
      "    Number of classes: 7\n",
      "    Number of train samples: 1624\n",
      "    Number of val samples: 274\n",
      "    Number of test samples: 270\n",
      "    Dataset mode: hard\n",
      "    Feature range: [0.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(name='grb-cora', data_dir='data/grb-cora', mode='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grb.model.torch import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(in_features=dataset.num_features,\n",
    "            out_features=dataset.num_classes,\n",
    "            hidden_features=[64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshitsinha3/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/Projects/GraphUnlearning/.venv/lib/python3.11/site-packages/grb/utils/utils.py:40: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:620.)\n",
      "  adj_tensor = torch.sparse.FloatTensor(sparse_concat.t(), sparse_data, torch.Size(adj.shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train loss 55.0298 | Train score 0.0844 | Val loss 49.5411 | Val score 0.1277\n",
      "Epoch 00010 | Best validation score: 0.4234\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint.pt'.\n",
      "Epoch 00010 | Train loss 14.2097 | Train score 0.4181 | Val loss 11.6524 | Val score 0.4234\n",
      "Epoch 00020 | Best validation score: 0.4270\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint.pt'.\n",
      "Epoch 00020 | Train loss 9.3462 | Train score 0.4021 | Val loss 9.1848 | Val score 0.4270\n",
      "Epoch 00030 | Best validation score: 0.4708\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint.pt'.\n",
      "Epoch 00030 | Train loss 3.9314 | Train score 0.4606 | Val loss 2.1169 | Val score 0.4708\n",
      "Epoch 00040 | Train loss 1.7718 | Train score 0.4760 | Val loss 1.9258 | Val score 0.4307\n",
      "Epoch 00050 | Best validation score: 0.4745\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint.pt'.\n",
      "Epoch 00050 | Train loss 1.4247 | Train score 0.5166 | Val loss 1.5684 | Val score 0.4745\n",
      "Epoch 00060 | Best validation score: 0.4854\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint.pt'.\n",
      "Epoch 00060 | Train loss 1.4805 | Train score 0.4858 | Val loss 1.7157 | Val score 0.4854\n",
      "Epoch 00070 | Best validation score: 0.5876\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint.pt'.\n",
      "Epoch 00070 | Train loss 1.3747 | Train score 0.5357 | Val loss 1.3950 | Val score 0.5876\n",
      "Epoch 00080 | Train loss 1.3185 | Train score 0.5831 | Val loss 1.4330 | Val score 0.5620\n",
      "Epoch 00090 | Best validation score: 0.6277\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint.pt'.\n",
      "Epoch 00090 | Train loss 1.2383 | Train score 0.5733 | Val loss 1.3237 | Val score 0.6277\n",
      "Epoch 00100 | Train loss 1.1736 | Train score 0.5837 | Val loss 1.4582 | Val score 0.5401\n",
      "Epoch 00110 | Train loss 1.1264 | Train score 0.5979 | Val loss 1.3777 | Val score 0.6058\n",
      "Epoch 00120 | Train loss 1.1311 | Train score 0.6121 | Val loss 1.4842 | Val score 0.6131\n",
      "Epoch 00130 | Train loss 1.0299 | Train score 0.6410 | Val loss 1.4255 | Val score 0.5949\n",
      "Epoch 00140 | Best validation score: 0.6496\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint.pt'.\n",
      "Epoch 00140 | Train loss 1.0198 | Train score 0.6466 | Val loss 1.3144 | Val score 0.6496\n",
      "Epoch 00150 | Train loss 1.0096 | Train score 0.6626 | Val loss 1.2055 | Val score 0.6460\n",
      "Epoch 00160 | Best validation score: 0.6606\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint.pt'.\n",
      "Epoch 00160 | Train loss 1.0146 | Train score 0.6429 | Val loss 1.3984 | Val score 0.6606\n",
      "Epoch 00170 | Best validation score: 0.6825\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint.pt'.\n",
      "Epoch 00170 | Train loss 0.9391 | Train score 0.6829 | Val loss 1.2121 | Val score 0.6825\n",
      "Epoch 00180 | Train loss 0.9466 | Train score 0.6761 | Val loss 1.2983 | Val score 0.6752\n",
      "Epoch 00190 | Train loss 0.9413 | Train score 0.6903 | Val loss 1.3222 | Val score 0.6423\n",
      "Model saved in './tmp_2024_05_02_16_21_47/checkpoint_final.pt'.\n"
     ]
    }
   ],
   "source": [
    "adam = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(dataset=dataset, optimizer=adam, loss=torch.nn.functional.nll_loss)\n",
    "trainer.train(model=model, n_epoch=200, dropout=0.5, train_mode=\"inductive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attacking: Sequential inject 4/20 nodes\n",
      "Attacking: Epoch 9, Loss: 5.86623, Surrogate test acc: 0.98889\n",
      "Attacking: Sequential inject 8/20 nodes\n",
      "Attacking: Epoch 9, Loss: 5.82277, Surrogate test acc: 0.97037\n",
      "Attacking: Sequential inject 12/20 nodes\n",
      "Attacking: Epoch 9, Loss: 5.78868, Surrogate test acc: 0.95185\n",
      "Attacking: Sequential inject 16/20 nodes\n",
      "Attacking: Epoch 9, Loss: 5.76347, Surrogate test acc: 0.93333\n",
      "Attacking: Sequential inject 20/20 nodes\n",
      "Attacking: Epoch 9, Loss: 5.73217, Surrogate test acc: 0.92593\n"
     ]
    }
   ],
   "source": [
    "from grb.attack.tdgia import TDGIA\n",
    "\n",
    "# Attack configuration\n",
    "tdgia = TDGIA(\n",
    "    lr=0.01,\n",
    "    n_epoch=10,\n",
    "    n_inject_max=20,\n",
    "    n_edge_max=20,\n",
    "    feat_lim_min=-0.9,\n",
    "    feat_lim_max=0.9,\n",
    "    sequential_step=0.2,\n",
    ")\n",
    "\n",
    "# covert adj to coo format\n",
    "adj = dataset.adj.tocoo()\n",
    "\n",
    "# Apply attack\n",
    "rst = tdgia.attack(\n",
    "    model=model,\n",
    "    adj=adj,\n",
    "    features=dataset.features,\n",
    "    target_mask=dataset.test_mask,\n",
    "    adj_norm_func=GCNAdjNorm,\n",
    ")\n",
    "# Get modified adj and features\n",
    "adj_attack, features_attack = rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'evaluate' from 'grb.utils' (/Users/akshitsinha3/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/Projects/GraphUnlearning/.venv/lib/python3.11/site-packages/grb/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m      3\u001b[0m test_score \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     adj\u001b[38;5;241m=\u001b[39madj_attack,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     adj_norm_func\u001b[38;5;241m=\u001b[39mGCNAdjNorm,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m test_score\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'evaluate' from 'grb.utils' (/Users/akshitsinha3/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/Projects/GraphUnlearning/.venv/lib/python3.11/site-packages/grb/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from grb.utils import evaluate\n",
    "\n",
    "test_score = evaluate(\n",
    "    model=model,\n",
    "    adj=adj_attack,\n",
    "    features=features_attack,\n",
    "    labels=dataset.labels,\n",
    "    mask=dataset.test_mask,\n",
    "    adj_norm_func=GCNAdjNorm,\n",
    ")\n",
    "\n",
    "test_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
